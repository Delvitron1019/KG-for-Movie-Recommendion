{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61312899-74d2-46ae-a7f5-15a8c8e813aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  10   10   10 ... 1881 1965 3171]\n",
      "[  1   1   1 ... 767 778 981]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "TPS_DIR = '../data/amazon/10-core/'\n",
    "np.random.seed(2022)\n",
    "\n",
    "def get_count(tp, id):\n",
    "    playcount_groupbyid = tp[[id, 'ratings']].groupby(id, as_index=True)\n",
    "    count = playcount_groupbyid.size()\n",
    "    return count\n",
    "\n",
    "\n",
    "def numerize(tp, user2id, item2id):\n",
    "    uid = list(map(lambda x: user2id[x], tp['user_id']))\n",
    "    sid = list(map(lambda x: item2id[x], tp['item_id']))\n",
    "    tp['user_id'] = uid\n",
    "    tp['item_id'] = sid\n",
    "    return tp\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = pd.read_csv(os.path.join(TPS_DIR, 'amazon.csv'))\n",
    "    usercount, itemcount = get_count(\n",
    "        data, 'user_id'), get_count(data, 'item_id')\n",
    "    unique_uid = usercount.index\n",
    "    unique_sid = itemcount.index\n",
    "    item2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
    "    user2id = dict((uid, i) for (i, uid) in enumerate(unique_uid))\n",
    "    data = numerize(data, user2id, item2id)\n",
    "    tp_rating = data[['user_id', 'item_id', 'ratings']]\n",
    "\n",
    "    n_ratings = tp_rating.shape[0]\n",
    "    test = np.random.choice(n_ratings, size=int(\n",
    "        0.20 * n_ratings), replace=False)\n",
    "    test_idx = np.zeros(n_ratings, dtype=bool)\n",
    "    test_idx[test] = True\n",
    "\n",
    "    tp_1 = tp_rating[test_idx]\n",
    "    tp_train = tp_rating[~test_idx]\n",
    "\n",
    "    data_test = data[test_idx]\n",
    "    data_train_valid = data[~test_idx]\n",
    "\n",
    "    n_ratings = tp_1.shape[0]\n",
    "    test = np.random.choice(n_ratings, size=int(\n",
    "        0.50 * n_ratings), replace=False)\n",
    "\n",
    "    test_idx = np.zeros(n_ratings, dtype=bool)\n",
    "    test_idx[test] = True\n",
    "\n",
    "    tp_test = tp_1[test_idx]\n",
    "    tp_valid = tp_1[~test_idx]\n",
    "\n",
    "    tp_train.to_csv(os.path.join(TPS_DIR, 'amazon_train.dat'),\n",
    "                    index=False, header=None)\n",
    "    tp_valid.to_csv(os.path.join(TPS_DIR, 'amazon_valid.dat'),\n",
    "                    index=False, header=None)\n",
    "    tp_test.to_csv(os.path.join(TPS_DIR, 'amazon_test.dat'),\n",
    "                    index=False, header=None)\n",
    "\n",
    "\n",
    "    item2entity_id = data[['item_id']].drop_duplicates()\n",
    "    user2entity_id = data[['user_id']].drop_duplicates()\n",
    "\n",
    "    review2entity = data_train_valid[['user_id', 'item_id', 'review_identifiers']].drop_duplicates()\n",
    "    review_user2entity = data_train_valid[['user_id', 'review_identifiers']].drop_duplicates()\n",
    "    review_item2entity = data_train_valid[['item_id', 'review_identifiers']].drop_duplicates()\n",
    "    review_users = list(set(review_user2entity['user_id'].values))\n",
    "    review_items = list(set(review_item2entity['item_id'].values))\n",
    "    for i in data_test.values:\n",
    "        if i[0] not in review_users:\n",
    "            s_u = pd.Series([i[0], ''])\n",
    "            s_u.index = ['user_id','review_identifiers']\n",
    "            review_user2entity = pd.concat([review_user2entity, s_u], ignore_index=True)\n",
    "        if i[1] not in review_items:\n",
    "            s_i = pd.Series([i[1], ''])\n",
    "            s_i.index = ['item_id','review_identifiers']\n",
    "            review_item2entity = pd.concat([review_item2entity,s_i], ignore_index=True)\n",
    "\n",
    "    item2entity_id.to_csv(os.path.join(TPS_DIR, 'item_index2entity_id.txt'), index=False, header=None)\n",
    "    user2entity_id.to_csv(os.path.join(TPS_DIR, 'user_index2entity_id.txt'), index=False, header=None)\n",
    "    review_user2entity.to_csv(os.path.join(TPS_DIR, 'review_user2entity.txt'), index=False, header=None)\n",
    "    review_item2entity.to_csv(os.path.join(TPS_DIR, 'review_item2entity.txt'), index=False, header=None)\n",
    "\n",
    "    usercount, itemcount = get_count(\n",
    "        data, 'user_id'), get_count(data, 'item_id')\n",
    "\n",
    "    print(np.sort(np.array(usercount.values)))\n",
    "\n",
    "    print(np.sort(np.array(itemcount.values)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3b45b83-e4b3-4dca-b07c-b4a215d985a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-d D]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\dnts5\\AppData\\Roaming\\jupyter\\runtime\\kernel-6a2a9ca5-cc20-492c-afd4-d7d215b30250.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "from numpy.lib.shape_base import split\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "RATING_FILE_NAME = dict({'imdb':'ratings.dat',\n",
    "                         'amazon':'ratings.dat'})\n",
    "SEP = dict({'imdb': ',', 'amazon': ','})\n",
    "THRESHOLD = dict({'imdb':8, 'amazon':4})\n",
    "# the direction of dataset.\n",
    "TPS_DIR = '../data/amazon/10-core/'\n",
    "\n",
    "# fusing users/items and their corresponding review entities into a heterogeneous graph.\n",
    "def convert_review_to_entity_id_file():\n",
    "    file_user = TPS_DIR + 'review_user2entity.txt'\n",
    "    print('reading user index to entity id file: ' + file_user + ' ...')\n",
    "    file_item = TPS_DIR + 'review_item2entity.txt'\n",
    "    print('reading item index to entity id file: ' + file_item+ ' ...')\n",
    "    file_item_id = TPS_DIR + 'item_index2entity_id.txt'\n",
    "    print('reading item id index to entity id file: ' + file_item_id+ ' ...')\n",
    "    file_user_id = TPS_DIR + 'user_index2entity_id.txt'\n",
    "    print('reading user id index to entity id file: ' + file_user_id+ ' ...')\n",
    "    i = 0\n",
    "    user_list = []\n",
    "    item_list = []\n",
    "    user_entity_list = []\n",
    "    item_entity_list = []\n",
    "    user4kg_ids = []\n",
    "    item4kg_ids = []\n",
    "    user_review_entity_list = []\n",
    "    item_review_entity_list = []\n",
    "    for item_id_line in open(file_item_id, encoding='utf-8').readlines():\n",
    "        item_id = item_id_line.strip().split(',')[0]\n",
    "        if 'i' + item_id not in entity_id2index:\n",
    "            item_index_old2new[item_id] = i\n",
    "            entity_id2index['i' + item_id] = i\n",
    "            # entity_index2id[i] = item_id\n",
    "            i += 1\n",
    "        item_list.append(item_id)\n",
    "        item_entity_list.append(entity_id2index['i'+item_id])\n",
    "    items_num = i\n",
    "    print(item_index_old2new['2737'])\n",
    "    print(\"the number of item is: \", items_num)\n",
    "    for item_line in open(file_item, encoding='utf-8').readlines():\n",
    "        if item_line.strip().split(',')[0]=='':\n",
    "            continue\n",
    "        item_index = str(int(float(item_line.strip().split(',')[0])))\n",
    "        item_satori_id = item_line.strip().split(',', 1)[1]\n",
    "        if item_satori_id != '':\n",
    "            item_satori_id = item_satori_id.strip('\\\"').split(',')\n",
    "            for entity in item_satori_id:\n",
    "                if entity not in entity_id2index:\n",
    "                    entity_id2index[entity] = i\n",
    "                    # entity_index2id[i] = entity\n",
    "                    i+=1\n",
    "                item4kg_ids.append(item_index_old2new[item_index])\n",
    "                item_review_entity_list.append(entity_id2index[entity])\n",
    "\n",
    "    item_review_convert = pd.DataFrame({'item_entity_inx': pd.Series(item4kg_ids),\n",
    "                                'review_entity_index': pd.Series(item_review_entity_list)})[['item_entity_inx', 'review_entity_index']]\n",
    "    item_id_convert = pd.DataFrame({'item_id': pd.Series(item_list),\n",
    "                                'entity_index': pd.Series(item_entity_list)})[['item_id', 'entity_index']]\n",
    "    item_review_convert.to_csv(os.path.join(TPS_DIR, 'review_item2entity_final.txt'), index=False, header=None)\n",
    "    item_id_convert.to_csv(os.path.join(TPS_DIR, 'item2entity_final.txt'), index=False, header=None)\n",
    "    user_start_index = i\n",
    "    user_start_index_doc = open(TPS_DIR + '/user_start_index.txt', 'w', encoding='utf-8')\n",
    "    user_start_index_doc.write('%d' % (user_start_index))\n",
    "    user_start_index_doc.close()\n",
    "\n",
    "    start_user_inx = i\n",
    "    for user_id_line in open(file_user_id, encoding='utf-8').readlines():\n",
    "        user_id = user_id_line.strip().split(',')[0]\n",
    "        if 'u' + user_id not in entity_id2index:\n",
    "            user_index_old2new[user_id] = i\n",
    "            entity_id2index['u' + user_id] = i\n",
    "            i += 1\n",
    "        user_list.append(user_id)\n",
    "        user_entity_list.append(entity_id2index['u'+user_id])\n",
    "    users_num = i-start_user_inx\n",
    "    print(\"the number of users is: \", users_num)\n",
    "    for user_line in open(file_user, encoding='utf-8').readlines():\n",
    "        user_index = user_line.strip().split(',')[0]\n",
    "        satori_id = user_line.strip().split(',', 1)[1]\n",
    "        if satori_id !='':\n",
    "            satori_id = satori_id.strip('\\\"').split(',')\n",
    "            for user_satori_id in satori_id:\n",
    "                if user_satori_id not in entity_id2index:\n",
    "                    entity_id2index[user_satori_id] = i\n",
    "                    i += 1\n",
    "                user4kg_ids.append(user_index_old2new[user_index])\n",
    "                user_review_entity_list.append(entity_id2index[user_satori_id])\n",
    "\n",
    "    user_review_convert = pd.DataFrame({'user_entity_inx': pd.Series(user4kg_ids),\n",
    "                                'review_entity_index': pd.Series(user_review_entity_list)})[['user_entity_inx', 'review_entity_index']]\n",
    "    user_id_convert = pd.DataFrame({'user_id': pd.Series(user_list),\n",
    "                                'entity_index': pd.Series(user_entity_list)})[['user_id', 'entity_index']]\n",
    "    user_review_convert.to_csv(os.path.join(TPS_DIR, 'review_user2entity_final.txt'), index=False, header=None)\n",
    "    user_id_convert.to_csv(os.path.join(TPS_DIR, 'user2entity_final.txt'), index=False, header=None)\n",
    "    np.save(os.path.join(TPS_DIR, 'entity_id2index.npy'), entity_id2index)\n",
    "\n",
    "    # combine all of the user review entity idexes and item review entity indexes into an array for kg edges negative sampling.\n",
    "    user_review_entity_list.extend(item_review_entity_list)\n",
    "    review_entity_list = list(set(user_review_entity_list))\n",
    "    if 'PAD' in review_entity_list:\n",
    "        review_entity_list.remove('PAD')\n",
    "    np.save(os.path.join(TPS_DIR, 'review_entity_list.npy'), review_entity_list)\n",
    "    \n",
    "    return users_num, items_num\n",
    "\n",
    "# add by liu at 20200908\n",
    "def convert_rating():\n",
    "    file_train = TPS_DIR + '/amazon_train.dat'\n",
    "    file_valid = TPS_DIR + '/amazon_valid.dat'\n",
    "    file_test = TPS_DIR + '/amazon_test.dat'\n",
    "    print('reading rating file...')\n",
    "    writer_train = open(TPS_DIR + '/amazon_train_final.txt', 'w', encoding='utf-8')\n",
    "    writer_valid = open(TPS_DIR + '/amazon_valid_final.txt', 'w', encoding='utf-8')\n",
    "    writer_test = open(TPS_DIR + '/amazon_test_final.txt', 'w', encoding='utf-8')\n",
    "    sample_num = 0\n",
    "    for line in open(file_train, encoding='utf-8').readlines():\n",
    "        array = line.strip().split(SEP[DATASET])\n",
    "        user_index_old = array[0]\n",
    "        item_index_old = array[1]\n",
    "        # if item_index_old not in item_index_old2new:  # the item is not in the final item set\n",
    "        #     continue\n",
    "        item_index = item_index_old2new[item_index_old]\n",
    "        user_index = user_index_old2new[user_index_old]\n",
    "        rating = float(array[2])\n",
    "        sample_num = sample_num + 1\n",
    "        writer_train.write('%d\\t%d\\t%d\\n' % (user_index, item_index, rating))\n",
    "    writer_train.close()\n",
    "    for line in open(file_test, encoding='utf-8').readlines():\n",
    "        array = line.strip().split(SEP[DATASET])\n",
    "        user_index_old = array[0]\n",
    "        item_index_old = array[1]\n",
    "        # if item_index_old not in item_index_old2new:  # the item is not in the final item set\n",
    "        #     continue\n",
    "        item_index = item_index_old2new[item_index_old]\n",
    "        user_index = user_index_old2new[user_index_old]\n",
    "        rating = float(array[2])\n",
    "        sample_num = sample_num + 1\n",
    "        writer_test.write('%d\\t%d\\t%d\\n' % (user_index, item_index, rating))\n",
    "    writer_test.close()\n",
    "    for line in open(file_valid, encoding='utf-8').readlines():\n",
    "        array = line.strip().split(SEP[DATASET])\n",
    "        user_index_old = array[0]\n",
    "        item_index_old = array[1]\n",
    "        # if item_index_old not in item_index_old2new:  # the item is not in the final item set\n",
    "        #     continue\n",
    "        item_index = item_index_old2new[item_index_old]\n",
    "        user_index = user_index_old2new[user_index_old]\n",
    "        rating = float(array[2])\n",
    "        sample_num = sample_num + 1\n",
    "        writer_valid.write('%d\\t%d\\t%d\\n' % (user_index, item_index, rating))\n",
    "    writer_valid.close()\n",
    "    print(\"sample number is: \", sample_num)\n",
    "\n",
    "def convert_kg():\n",
    "    print('converting kg.txt file ...')\n",
    "    entity_cnt = len(entity_id2index)\n",
    "    edge_num = 0\n",
    "    kg_dict = dict()\n",
    "\n",
    "    writer = open(TPS_DIR + '/kg_final.txt', 'w', encoding='utf-8')\n",
    "    user_review_triple = open(os.path.join(TPS_DIR, 'review_user2entity_final.txt'), encoding='utf-8')\n",
    "    item_review_triple = open(os.path.join(TPS_DIR, 'review_item2entity_final.txt'), encoding='utf-8')\n",
    "\n",
    "    # add the triples of movie-review, user-reviews.\n",
    "    for user_triple in user_review_triple:\n",
    "        user_head = int(user_triple.strip().split(',')[0])\n",
    "        user_review_tail = int(user_triple.strip().split(',')[1])\n",
    "        if user_head not in kg_dict:\n",
    "            kg_dict[user_head]=[]\n",
    "        if user_review_tail not in kg_dict[user_head]:\n",
    "            edge_num = edge_num + 1\n",
    "            kg_dict[user_head].append(user_review_tail)\n",
    "            writer.write('%d\\t%d\\n' % (user_head, user_review_tail))\n",
    "    \n",
    "    for item_triple in item_review_triple:\n",
    "        item_head = int(item_triple.strip().split(',')[0])\n",
    "        item_review_tail = int(item_triple.strip().split(',')[1])\n",
    "        if item_head not in kg_dict:\n",
    "            kg_dict[item_head] = []\n",
    "        if item_review_tail not in kg_dict[item_head]:\n",
    "            edge_num = edge_num + 1\n",
    "            kg_dict[item_head].append(item_review_tail)\n",
    "\n",
    "            writer.write('%d\\t%d\\n' % (item_head, item_review_tail))\n",
    "\n",
    "    writer.close()\n",
    "    print('number of graph nodes (containing users and items): %d' % entity_cnt)\n",
    "    print(\"number of review entities: \", entity_cnt-users_num-items_num)\n",
    "    print('number of edges: %d' % edge_num)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    np.random.seed(555)\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-d', type=str, default='amazon', help='which dataset to preprocess')\n",
    "    args = parser.parse_args()\n",
    "    DATASET = args.d\n",
    "\n",
    "    entity_id2index = dict()\n",
    "    relation_id2index = dict()\n",
    "    item_index_old2new = dict()\n",
    "    user_index_old2new = dict()\n",
    "    tail_nodes = []\n",
    "\n",
    "    users_num, items_num = convert_review_to_entity_id_file()\n",
    "    convert_rating()\n",
    "    convert_kg()\n",
    "\n",
    "    print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278a2759-4795-4130-8e09-275ff07ca034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c71a24d-70dd-4fc0-8195-c604fe11dd39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
